{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "from math import sqrt\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n",
    "# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n",
    "\n",
    "if DEVICE != 'cuda':\n",
    "    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for loading the hidden dataset.\n",
    "\n",
    "def load_example(df_row):\n",
    "    image = torchvision.io.read_image(df_row['image_path'])\n",
    "    result = {\n",
    "        'image': image,\n",
    "        'image_id': df_row['image_id'],\n",
    "        'age_group': df_row['age_group'],\n",
    "        'age': df_row['age'],\n",
    "        'person_id': df_row['person_id']\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "class HiddenDataset(Dataset):\n",
    "    '''The hidden dataset.'''\n",
    "    def __init__(self, split='train'):\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "\n",
    "        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n",
    "        df['image_path'] = df['image_id'].apply(\n",
    "            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n",
    "        df = df.sort_values(by='image_path')\n",
    "        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n",
    "        if len(self.examples) == 0:\n",
    "            raise ValueError('No examples.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        image = example['image']\n",
    "        image = image.to(torch.float32)\n",
    "        example['image'] = image\n",
    "        return example\n",
    "\n",
    "\n",
    "def get_dataset(batch_size):\n",
    "    '''Get the dataset.'''\n",
    "    retain_ds = HiddenDataset(split='retain')\n",
    "    forget_ds = HiddenDataset(split='forget')\n",
    "    val_ds = HiddenDataset(split='validation')\n",
    "\n",
    "    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n",
    "    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return retain_loader, forget_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss_fn(outputs, dist_target):\n",
    "    kl_loss = F.kl_div(torch.log_softmax(outputs, dim=1), dist_target, log_target=True, reduction='batchmean')\n",
    "    return kl_loss\n",
    "\n",
    "def entropy_loss_fn(outputs, labels, dist_target, class_weights):\n",
    "    ce_loss = F.cross_entropy(outputs, labels, weight=class_weights)\n",
    "    entropy_dist_target = torch.sum(-torch.exp(dist_target) * dist_target, dim=1)\n",
    "    entropy_outputs = torch.sum(-torch.softmax(outputs, dim=1) * torch.log_softmax(outputs, dim=1), dim=1)\n",
    "    entropy_loss = F.mse_loss(entropy_outputs, entropy_dist_target)\n",
    "    return ce_loss + entropy_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace the below simple unlearning with your own unlearning function.\n",
    "\n",
    "def unlearning(\n",
    "    net, \n",
    "    retain_loader, \n",
    "    forget_loader, \n",
    "    val_loader,\n",
    "    class_weights=None,\n",
    "):\n",
    "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
    "    epochs = 3.2\n",
    "    max_iters = int(len(retain_loader) * epochs)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.0005,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "    initial_net = deepcopy(net)\n",
    "    \n",
    "    net.train()\n",
    "    initial_net.eval()\n",
    "    \n",
    "    def prune_model(net, amount=0.95, rand_init=True):\n",
    "        # Modules to prune\n",
    "        modules = list()\n",
    "        for k, m in enumerate(net.modules()):\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                modules.append((m, 'weight'))\n",
    "                if m.bias is not None:\n",
    "                    modules.append((m, 'bias'))\n",
    "\n",
    "        # Prune criteria\n",
    "        prune.global_unstructured(\n",
    "            modules,\n",
    "            #pruning_method=prune.RandomUnstructured,\n",
    "            pruning_method=prune.L1Unstructured,\n",
    "            amount=amount,\n",
    "        )\n",
    "\n",
    "        # Perform the prune\n",
    "        for k, m in enumerate(net.modules()):\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                prune.remove(m, 'weight')\n",
    "                if m.bias is not None:\n",
    "                    prune.remove(m, 'bias')\n",
    "\n",
    "        # Random initialization\n",
    "        if rand_init:\n",
    "            for k, m in enumerate(net.modules()):\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    mask = m.weight == 0\n",
    "                    c_in = mask.shape[1]\n",
    "                    k = 1/(c_in*mask.shape[2]*mask.shape[3])\n",
    "                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n",
    "                    m.weight.data[mask] = randinit[mask]\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    mask = m.weight == 0\n",
    "                    c_in = mask.shape[1]\n",
    "                    k = 1/c_in\n",
    "                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n",
    "                    m.weight.data[mask] = randinit[mask]\n",
    "    \n",
    "    num_iters = 0\n",
    "    running = True\n",
    "    prune_amount = 0.99\n",
    "    prune_model(net, prune_amount, True)\n",
    "    while running:\n",
    "        net.train()\n",
    "        for sample in retain_loader:\n",
    "            inputs = sample[\"image\"]\n",
    "            targets = sample[\"age_group\"]\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            # Get target distribution\n",
    "            with torch.no_grad():\n",
    "                original_outputs = initial_net(inputs)\n",
    "                preds = torch.log_softmax(original_outputs, dim=1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = entropy_loss_fn(outputs, targets, preds, class_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_iters += 1\n",
    "            # Stop at max iters\n",
    "            if num_iters > max_iters:\n",
    "                running = False\n",
    "                break\n",
    "        \n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n",
    "    # mock submission\n",
    "    net = resnet18(weights=None, num_classes=10)\n",
    "    for k, m in enumerate(net.modules()):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            prune.l1_unstructured(m, name=\"weight\", amount=0.95)\n",
    "            prune.remove(m, 'weight')\n",
    "            \n",
    "    print(m)\n",
    "    subprocess.run('touch submission.zip', shell=True)\n",
    "else:\n",
    "    \n",
    "    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n",
    "    # as otherwise this notebook may fail due to running out of disk space.\n",
    "    # The below code saves them in /kaggle/tmp to avoid that issue.\n",
    "    class_weights_fname = \"/kaggle/input/neurips-2023-machine-unlearning/age_class_weights.json\"\n",
    "    with open(class_weights_fname) as f:\n",
    "        # Returns JSON object as a dictionary(?)\n",
    "        class_weights = json.load(f)\n",
    "\n",
    "    # Remove any dictionary layers, if there are any(?)\n",
    "    while isinstance(class_weights, dict):\n",
    "        if len(class_weights) > 1:\n",
    "            # Assume each key maps to one weight, in the correct order\n",
    "            class_weights = list(class_weights.values())\n",
    "            break\n",
    "        for _, class_weights in class_weights.items():\n",
    "            # Strip away a dict layer and handle its contents, using the\n",
    "            # value from the first key in the dict only.\n",
    "            break\n",
    "\n",
    "    # We should now have a list\n",
    "    # if not isinstance(class_weights, list):\n",
    "    #     raise ValueError(f\"class_weights is a {type(class_weights)}, not a list\")\n",
    "\n",
    "    # Convert list of weights into a tensor\n",
    "    class_weights = torch.tensor(class_weights).to(DEVICE, dtype=torch.float32)\n",
    "    # The JSON file actually contains number of occurances. To correct for imbalance, the\n",
    "    # weighting should be the reciprocal of the count.\n",
    "    class_weights = class_weights ** -0.1\n",
    "    \n",
    "    os.makedirs('/kaggle/tmp', exist_ok=True)\n",
    "    retain_loader, forget_loader, validation_loader = get_dataset(64)\n",
    "    net = resnet18(weights=None, num_classes=10)\n",
    "    net.to(DEVICE)\n",
    "    for i in range(512):\n",
    "        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n",
    "        unlearning(net, retain_loader, forget_loader, validation_loader, class_weights=class_weights)\n",
    "        net = net.to(torch.half)\n",
    "        state = net.state_dict()\n",
    "        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n",
    "        net = net.to(torch.float)\n",
    "        \n",
    "    # Ensure that submission.zip will contain exactly 512 checkpoints \n",
    "    # (if this is not the case, an exception will be thrown).\n",
    "    unlearned_ckpts = os.listdir('/kaggle/tmp')\n",
    "    if len(unlearned_ckpts) != 512:\n",
    "        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n",
    "        \n",
    "    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
